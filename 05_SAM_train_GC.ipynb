{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f47d7ad-ed52-42be-8258-3c9392685387",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train SAM on Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b561d-be0d-4c0a-80e5-abec6c05a178",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up Modelling Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c430f2-7900-4891-8273-1328c39e204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-g7af9qoy\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-g7af9qoy\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rasterio\n",
      "  Obtaining dependency information for rasterio from https://files.pythonhosted.org/packages/5e/19/4617aaaf3166b06c520db50de38108bf069e63512712a7edda6710f4687b/rasterio-1.3.8.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached rasterio-1.3.8.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n",
      "Collecting monai\n",
      "  Obtaining dependency information for monai from https://files.pythonhosted.org/packages/08/94/e8a7ba00dd0c7ce959648b562043bd22125d65f5e519e566c822f71bc437/monai-1.3.0-202310121228-py3-none-any.whl.metadata\n",
      "  Using cached monai-1.3.0-202310121228-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/lib/python3.10/site-packages (2023.9.2)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting segment_anything\n",
      "  Using cached segment_anything-1.0-py3-none-any.whl (36 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Using cached affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from rasterio) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from rasterio) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.10/site-packages (from rasterio) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.23.5)\n",
      "Collecting snuggs>=1.4.1 (from rasterio)\n",
      "  Using cached snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from rasterio) (68.2.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: torch>=1.9 in /opt/conda/lib/python3.10/site-packages (from monai) (2.0.0+cu118)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (3.8.5)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: fsspec==2023.9.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2023.9.2)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.11.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (13.0.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/35/a8/36d8d7b3e46b377800d8dec47891cdf05842d1a2366909ae4a0c89fbc5e6/multiprocess-0.70.15-py310-none-any.whl.metadata\n",
      "  Using cached multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "  Using cached fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
      "  Using cached fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "  Using cached fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/66/f8/38298237d18d4b6a8ee5dfe390e97bed5adb8e01ec6f9680c0ddf3066728/datasets-2.14.4-py3-none-any.whl.metadata\n",
      "  Using cached datasets-2.14.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs) (1.26.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.10/site-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9->monai) (2.0.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.9->monai) (3.27.5)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.9->monai) (17.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (3.20.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
      "Using cached rasterio-1.3.8.post2-cp310-cp310-manylinux2014_x86_64.whl (20.6 MB)\n",
      "Using cached transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "Using cached monai-1.3.0-202310121228-py3-none-any.whl (1.3 MB)\n",
      "Using cached datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Building wheels for collected packages: segment-anything\n",
      "  Building wheel for segment-anything (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36586 sha256=2c44e910d765737cd994167d05afb017daf4cb01664b986792f2ea92da31747a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6sa513ec/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
      "Successfully built segment-anything\n",
      "Installing collected packages: segment-anything, xxhash, snuggs, safetensors, regex, dill, cligj, click-plugins, affine, rasterio, multiprocess, huggingface-hub, tokenizers, transformers, datasets, monai\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 datasets-2.14.4 dill-0.3.7 huggingface-hub-0.17.3 monai-1.3.0 multiprocess-0.70.15 rasterio-1.3.8.post2 regex-2023.10.3 safetensors-0.4.0 segment-anything-1.0 snuggs-1.4.7 tokenizers-0.14.1 transformers-4.34.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rasterio transformers tqdm monai gcsfs datasets segment_anything git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b17f855-0123-4547-9450-dfc83db826fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=30'  # Set it to an appropriate value\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6caa2364-defd-4886-9af2-57775cb5fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader, random_split\n",
    "from transformers import SamProcessor\n",
    "from transformers import SamModel\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import time\n",
    "import monai\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50294e-f2ba-4369-93e8-fff0b480a073",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Various Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783ae817-d34e-44d8-bd22-1196fbab7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    # input_dir = 'satellite_images/train/stack/'\n",
    "    # output_dir = 'models/'\n",
    "    input_dir = 'gs://meter-sam/train/stack/'\n",
    "    output_dir = 'gs://meter-sam/model/'\n",
    "    num_epochs = 10\n",
    "    batch_size = 4\n",
    "    learning_rate = 1e-5\n",
    "    weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91399d3-1722-4d4f-9c4e-5b2bebaa5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_arguments():\n",
    "#     \"\"\"Parse command-line arguments.\"\"\"\n",
    "#     parser = argparse.ArgumentParser(description=\"Train the SAM on Vertex AI.\")\n",
    "\n",
    "#     # Directories and File Paths\n",
    "#     parser.add_argument('--input-dir', type=str, default='gs://meter-sam/stack/', help='Input data directory path.')\n",
    "#     parser.add_argument('--output-dir', type=str, default='gs://meter-sam/model/', help='Output data directory path.')\n",
    "\n",
    "#     # Hyperparameters\n",
    "#     parser.add_argument('--num-epochs', type=int, default=5, help='Number of training epochs.')\n",
    "#     parser.add_argument('--batch-size', type=int, default=2, help='Batch size for training.')\n",
    "#     parser.add_argument('--learning-rate', type=float, default=1e-5, help='Learning rate for optimizer.')\n",
    "#     parser.add_argument('--weight-decay', type=float, default=0, help='Weight decay for optimizer.')\n",
    "    \n",
    "#     return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059ecd9d-50be-482d-975b-941ecaa8110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tiff(file_path):\n",
    "    \"\"\"Read TIFF file and return its content.\"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            return src.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {file_path}. Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71dd703-382e-4a44-8e26-5024c903b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "    \"\"\"Compute bounding box for the given ground truth map.\"\"\"\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01d2d70-54fb-4342-99de-ab69028442fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAMDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    This class is used to create a dataset that serves input images and masks.\n",
    "    It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2312d95-deb6-4396-9c24-a6b9d2787405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(args):\n",
    "    \"\"\"Prepare datasets for training.\"\"\"\n",
    "    images = read_tiff(args.input_dir + 'images.tif')\n",
    "    masks = read_tiff(args.input_dir + 'masks.tif')\n",
    "    # Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "    dataset_dict = {\n",
    "        \"image\": [Image.fromarray(img) for img in images],\n",
    "        \"label\": [Image.fromarray(mask) for mask in masks],\n",
    "    }\n",
    "    # Create the dataset using the datasets.Dataset class\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    # Initialize the processor\n",
    "    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    # Create an instance of the SAMDataset\n",
    "    train_dataset = SAMDataset(dataset=dataset, processor=processor)\n",
    "    return train_dataset, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37414a2f-e6f7-40dd-b08a-d8c8e524749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"Load SAM model.\"\"\"\n",
    "    model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "            param.requires_grad_(False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efecda39-fcd6-4e6b-ba29-5edbec30db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sam(train_dataset, model, processor, args):\n",
    "    \"\"\"Train the SAM model.\"\"\"\n",
    "    # # Splitting the dataset into training and validation\n",
    "    # train_size = int(0.9 * len(dataset))  # 90% for training\n",
    "    # val_size = len(dataset) - train_size   # remaining 10% for validation\n",
    "    # train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    # Create a DataLoader instance for the training dataset\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n",
    "    optimizer = torch.optim.Adam(model.mask_decoder.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "    # Training device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    # Start training\n",
    "    total_start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_losses = []\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            # forward passFar\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                multimask_output=False)\n",
    "            # compute loss\n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "            # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        print(f'EPOCH: {epoch}')\n",
    "        print(f'Mean loss: {mean(epoch_losses)}')\n",
    "        print(f'Time taken for the epoch: {elapsed_time:.2f} minutes\\n')\n",
    "    # Calculate total training time\n",
    "    total_training_time = (time.time() - total_start_time) / 60\n",
    "    print(f'Total training time: {total_training_time:.2f} minutes')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1dee0b-e056-4e3f-865d-4e5dd9ef458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, output_dir):\n",
    "    \"\"\"Save the trained model.\"\"\"\n",
    "    fs = gcsfs.GCSFileSystem(project='imposing-mind-398223')\n",
    "    with fs.open(output_dir+\"model_base.pth\", 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "    # torch.save(model.state_dict(), output_dir+\"model_base.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658b2c86-bd1a-4419-be60-2396f090dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     \"\"\"Main function to orchestrate model training.\"\"\"\n",
    "#     train_dataset, processor = prepare_datasets(args)\n",
    "#     model = load_model()\n",
    "#     model = train_sam(train_dataset, model, processor, args)\n",
    "#     save_model(model, args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3a81d-be9c-4dd1-bc88-d6d81474986d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load, Train, and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49d56df2-a5f2-4e15-8eae-5c37dce703e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, processor = prepare_datasets(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef0ab0a-a432-4942-8f32-a6685e078b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a161af6-c8c2-4aab-bc85-57adb4e9ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68a37a75-07d8-4508-aed9-d3e444192463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:12:33<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Mean loss: 0.6678856571742466\n",
      "Time taken for the epoch: 72.57 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:30<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "Mean loss: 0.647871703345435\n",
      "Time taken for the epoch: 71.50 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:35<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2\n",
      "Mean loss: 0.6412016136407852\n",
      "Time taken for the epoch: 71.60 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:37<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3\n",
      "Mean loss: 0.6364234496559416\n",
      "Time taken for the epoch: 71.63 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:34<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4\n",
      "Mean loss: 0.6328254467146738\n",
      "Time taken for the epoch: 71.58 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:33<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 5\n",
      "Mean loss: 0.6293139483554022\n",
      "Time taken for the epoch: 71.55 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:28<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 6\n",
      "Mean loss: 0.6268264428002494\n",
      "Time taken for the epoch: 71.47 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:30<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 7\n",
      "Mean loss: 0.6238799877234867\n",
      "Time taken for the epoch: 71.51 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:27<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 8\n",
      "Mean loss: 0.621107563996315\n",
      "Time taken for the epoch: 71.46 minutes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [1:11:22<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 9\n",
      "Mean loss: 0.6186118943384715\n",
      "Time taken for the epoch: 71.38 minutes\n",
      "\n",
      "Total training time: 71.38 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_sam(train_dataset, model, processor, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bcb1f58-c691-4807-8083-b66f02106869",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5216c863-2dc7-4f5e-ad2e-061e495d3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0 (Local)",
   "language": "python",
   "name": "local-pytorch-2-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
