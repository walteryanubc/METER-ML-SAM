{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:migration,new",
    "tags": []
   },
   "source": [
    "# 03 Train SAM on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62b666b5ce2b"
   },
   "source": [
    "Custom image classification with a custom training container\n",
    "\n",
    "SERVE_DOCKER_URI## Overview\n",
    "\n",
    "This notebook demonstrates how to train a custom image classification model by creating a custom training SAM with Pytorch and the Vertex AI SDK. The notebook also demonstrates how to deploys the trained model to Vertex AI and generate predictions from it.\n",
    "\n",
    "Learn more about [Migrate to Vertex AI](https://cloud.google.com/vertex-ai/docs/start/migrating-to-vertex-ai) and [Custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1ae7d54ad29",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to train SAM using a custom container and Vertex AI training. After training, you also deploy the model to Vertex AI using a pre-built container and generate both batch and online predictions on it. \n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- *Vertex AI Training*\n",
    "- *Vertex AI Model Registry*\n",
    "- *Vertex AI Batch Predictions*\n",
    "- *Vertex AI Endpoints*\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- *Package the training code into a python application.*\n",
    "- *Containerize the training application using Cloud Build and Artifact Registry.*\n",
    "- *Create a custom container training job in Vertex AI and run it.*\n",
    "- *Evaluate the model generated from the training job.*\n",
    "- *Create a model resource for the trained model in Vertex AI Model Registry.*\n",
    "- *Run a Vertex AI batch prediction job.*\n",
    "- *Deploy the model resource to a Vertex AI Endpoint.*\n",
    "- *Run a online prediction job on the model resource.*\n",
    "- *Clean up the resources created.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* Cloud Build\n",
    "* Artifact Registry\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), [Cloud Build pricing](https://cloud.google.com/build/pricing), [Artifact Registry pricing](https://cloud.google.com/artifact-registry/pricing) and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMHz63rPbq6P",
    "tags": []
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.35.0)\n",
      "Requirement already satisfied: google-cloud-storage in ./.local/lib/python3.10/site-packages (2.12.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.11.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in ./.local/lib/python3.10/site-packages (from google-cloud-storage) (2.23.3)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.60.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.58.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2023.7.22)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Collecting rasterio\n",
      "  Obtaining dependency information for rasterio from https://files.pythonhosted.org/packages/5e/19/4617aaaf3166b06c520db50de38108bf069e63512712a7edda6710f4687b/rasterio-1.3.8.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached rasterio-1.3.8.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: gcsfs in /opt/conda/lib/python3.10/site-packages (2023.9.2)\n",
      "Requirement already satisfied: google-auth in ./.local/lib/python3.10/site-packages (2.23.3)\n",
      "Collecting affine (from rasterio)\n",
      "  Using cached affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.10/site-packages (from rasterio) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from rasterio) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.10/site-packages (from rasterio) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rasterio) (1.23.5)\n",
      "Collecting snuggs>=1.4.1 (from rasterio)\n",
      "  Using cached snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from rasterio) (68.2.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (3.8.5)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: fsspec==2023.9.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2023.9.2)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-storage in ./.local/lib/python3.10/site-packages (from gcsfs) (2.12.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from gcsfs) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.10/site-packages (from snuggs>=1.4.1->rasterio) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (2.6.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs) (1.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->gcsfs) (1.26.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs) (3.20.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Using cached rasterio-1.3.8.post2-cp310-cp310-manylinux2014_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: snuggs, cligj, click-plugins, affine, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.3.8.post2 snuggs-1.4.7\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                         google-cloud-storage --user\n",
    "\n",
    "! pip3 install rasterio gcsfs google-auth\n",
    "\n",
    "# if os.getenv(\"IS_TESTING\"):\n",
    "#     ! apt-get update && apt-get install -y python3-opencv-headless --user\n",
    "#     ! apt-get install -y libgl1-mesa-dev --user\n",
    "#     ! pip3 install --upgrade opencv-python-headless --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "PROJECT_ID = \"imposing-mind-398223\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Set the region\n",
    "\n",
    "**Optional**: Update the 'REGION' variable to specify the region that you want to use. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nsN5NJKSu-GU"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "# BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://meter-sam-imposing-mind-398223-unique\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Load the Vertex AI SDK and other libraries for Python used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform\n",
    "import os\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk",
    "tags": []
   },
   "source": [
    "## Configure containers for training and prediction\n",
    "\n",
    "In this step, you set the configuration parameters used while creating training and serving containers like number and type of GPUs needed(by default CPU is used) and machine-type need for serving.\n",
    "\n",
    "\n",
    "### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for training and prediction.\n",
    "\n",
    "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more [here](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators) hardware accelerator support for your region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk"
   },
   "outputs": [],
   "source": [
    "TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction",
    "tags": []
   },
   "source": [
    "### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training and prediction.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you use for for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "machine:training,prediction"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-2\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MACHINE_TYPE = \"n1-standard\"\n",
    "TRAIN_VCPU = \"4\"\n",
    "\n",
    "TRAIN_COMPUTE = TRAIN_MACHINE_TYPE + \"-\" + TRAIN_VCPU\n",
    "print(\"Train machine type\", TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "DEPLOY_MACHINE_TYPE = \"n1-standard\"\n",
    "DEPLOY_VCPU = \"2\"\n",
    "\n",
    "DEPLOY_COMPUTE = DEPLOY_MACHINE_TYPE + \"-\" + DEPLOY_VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examine_training_package",
    "tags": []
   },
   "source": [
    "## Package training application\n",
    "\n",
    "In this step, you package the code for training SAM using your own custom container.\n",
    "\n",
    "To use your own custom container, you build a Docker file. First, you create a directory for the container components.\n",
    "\n",
    "### Package layout\n",
    "\n",
    "Before you start the training, you look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - train.py\n",
    "\n",
    "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file `trainer/task.py` is the Python script for executing the custom training job. \n",
    "\n",
    "*Note: When the file is referred in the worker pool specification, the directory slash is replaced with a dot (`trainer.train`) and dropped the file suffix (`.py`).*\n",
    "\n",
    "### Package Assembly\n",
    "\n",
    "In the following cells, you assemble the training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "examine_training_package"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training and serving script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "! mkdir custom/train\n",
    "! mkdir custom/serve\n",
    "\n",
    "# Add package information\n",
    "! touch custom/train/README.md\n",
    "! touch custom/serve/README.md\n",
    "\n",
    "# Add required libary information\n",
    "requirements_train = \"\"\"git+https://github.com/facebookresearch/segment-anything.git\n",
    "transformers\n",
    "datasets\n",
    "monai\n",
    "rasterio\n",
    "segment_anything\n",
    "\"\"\"\n",
    "! echo \"$requirements_train\" > custom/train/requirements.txt\n",
    "\n",
    "requirements_serve = \"\"\"transformers\n",
    "torch\n",
    "numpy\n",
    "Pillow\n",
    "rasterio\n",
    "gcsfs\n",
    "fastapi\n",
    "uvicorn\n",
    "\"\"\"\n",
    "! echo \"$requirements_serve\" > custom/serve/requirements.txt\n",
    "\n",
    "# setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "# ! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "# setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "# ! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "# pkg_info = \"Metadata-Version: 1.0\\n\\nName: CIFAR10 image classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "# ! echo \"$pkg_info\" > custom/PKG-INFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:cifar10",
    "tags": []
   },
   "source": [
    "### Contents of trainer task\n",
    "\n",
    "In the next cell, you write the contents of the training script `task.py`. The script's details are not discussed in depth here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "taskpy_contents:cifar10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/train/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/train/trainer.py\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import argparse\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from transformers import SamProcessor\n",
    "from transformers import SamModel\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "import time\n",
    "import monai\n",
    "import gcsfs\n",
    "# from google.cloud import storage\n",
    "# from io import BytesIO\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse command-line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Train the SAM on Vertex AI.\")\n",
    "\n",
    "    # Directories and File Paths\n",
    "    parser.add_argument('--input-dir', type=str, default='gs://meter-sam/train/stack/', help='Input data directory path.')\n",
    "    parser.add_argument('--output-dir', type=str, default='gs://meter-sam/model/', help='Output data directory path.')\n",
    "\n",
    "    # Hyperparameters\n",
    "    parser.add_argument('--num-epochs', type=int, default=10, help='Number of training epochs.')\n",
    "    parser.add_argument('--batch-size', type=int, default=4, help='Batch size for training.')\n",
    "    parser.add_argument('--learning-rate', type=float, default=1e-5, help='Learning rate for optimizer.')\n",
    "    parser.add_argument('--weight-decay', type=float, default=0, help='Weight decay for optimizer.')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def read_tiff(file_path):\n",
    "    \"\"\"Read TIFF file and return its content.\"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            return src.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {file_path}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "    \"\"\"Compute bounding box for the given ground truth map.\"\"\"\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "class SAMDataset(TorchDataset):\n",
    "    \"\"\"\n",
    "    This class is used to create a dataset that serves input images and masks.\n",
    "    It takes a dataset and a processor as input and overrides the __len__ and __getitem__ methods of the Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        prompt = get_bounding_box(ground_truth_mask)\n",
    "        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "        return inputs\n",
    "\n",
    "def prepare_datasets(args):\n",
    "    \"\"\"Prepare datasets for training.\"\"\"\n",
    "    images = read_tiff(args.input_dir + 'images.tif')\n",
    "    masks = read_tiff(args.input_dir + 'masks.tif')\n",
    "    # Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "    dataset_dict = {\n",
    "        \"image\": [Image.fromarray(img) for img in images],\n",
    "        \"label\": [Image.fromarray(mask) for mask in masks],\n",
    "    }\n",
    "    # Create the dataset using the datasets.Dataset class\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    # Initialize the processor\n",
    "    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    # Create an instance of the SAMDataset\n",
    "    train_dataset = SAMDataset(dataset=dataset, processor=processor)\n",
    "    return train_dataset, processor\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load SAM model.\"\"\"\n",
    "    model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "            param.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "def train_sam(train_dataset, model, processor, args):\n",
    "    \"\"\"Train the SAM model.\"\"\"\n",
    "    # Create a DataLoader instance for the training dataset\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=False, num_workers=4, pin_memory=True)\n",
    "    optimizer = torch.optim.Adam(model.mask_decoder.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n",
    "    # Training device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for epoch in range(args.num_epochs):\n",
    "        total_start_time = time.time()\n",
    "        epoch_losses = []\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            # forward passFar\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                multimask_output=False)\n",
    "            # compute loss\n",
    "            predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "            loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "            # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        print(f'EPOCH: {epoch}')\n",
    "        print(f'Mean loss: {mean(epoch_losses)}')\n",
    "        print(f'Time taken for the epoch: {elapsed_time:.2f} minutes\\n')\n",
    "    # Calculate total training time\n",
    "    total_training_time = (time.time() - total_start_time) / 60\n",
    "    print(f'Total training time: {total_training_time:.2f} minutes')   \n",
    "    return model\n",
    "\n",
    "def save_model(model, output_dir):\n",
    "    \"\"\"Save the trained model.\"\"\"\n",
    "    fs = gcsfs.GCSFileSystem(project='imposing-mind-398223')\n",
    "    with fs.open(output_dir+\"model.pth\", 'wb') as f:\n",
    "        torch.save(model.state_dict(), f)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate model training.\"\"\"\n",
    "    args = parse_arguments()\n",
    "    train_dataset, processor = prepare_datasets(args)\n",
    "    model = load_model()\n",
    "    model = train_sam(train_dataset, model, processor, args)\n",
    "    save_model(model, args.output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# # Load SAM from Google Cloud Service\n",
    "    \n",
    "# def load_model_from_gcs(bucket_name, blob_path):\n",
    "#     \"\"\"Load SAM from Google Cloud Storage.\"\"\"\n",
    "#     # Initialize a GCS client\n",
    "#     client = storage.Client()\n",
    "#     bucket = client.get_bucket(bucket_name)\n",
    "#     blob = bucket.blob(blob_path)\n",
    "\n",
    "#     # Download the blob contents into a BytesIO object\n",
    "#     buffer = io.BytesIO()\n",
    "#     blob.download_to_file(buffer)\n",
    "#     buffer.seek(0)\n",
    "\n",
    "#     # Load the model directly from the BytesIO object\n",
    "#     state_dict = torch.load(buffer)\n",
    "#     model = sam_model_registry[\"vit_h\"]()\n",
    "#     model.load_state_dict(state_dict)\n",
    "#     return model\n",
    "\n",
    "# # Hyperparameter tuning for Pytorch\n",
    "\n",
    "# $pip install hypertune\n",
    "# import hypertune\n",
    "\n",
    "# # Define metric\n",
    "# hp_metric = history.history['val_accuracy'][-1]\n",
    "\n",
    "# # Report the metric to hypertune\n",
    "# hpt = hypertune.HyperTune()\n",
    "# hpt.report_hyperparameter_tuning_metric(\n",
    "#     hyperparameter_metric_tag='accuracy', # metric name\n",
    "#     metric_value=hp_metric, #metric value\n",
    "#     global_step=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:cifar10",
    "tags": []
   },
   "source": [
    "### Contents of sever task\n",
    "\n",
    "In the next cell, you write the contents of the serving script `app.py`. The script's details are not discussed in depth here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/serve/server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/serve/server.py\n",
    "import os\n",
    "import numpy as np\n",
    "import uvicorn\n",
    "import logging\n",
    "import torch\n",
    "from fastapi import FastAPI, Request, Response, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "from transformers import SamConfig, SamProcessor, SamModel\n",
    "from PIL import Image\n",
    "import gcsfs\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from typing import List, Dict\n",
    "\n",
    "# Configuration\n",
    "AIP_HEALTH_ROUTE = os.environ.get('AIP_HEALTH_ROUTE', '/health')\n",
    "AIP_PREDICT_ROUTE = os.environ.get('AIP_PREDICT_ROUTE', '/predict')\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialization of model, processor, etc.\n",
    "fs = gcsfs.GCSFileSystem(project='imposing-mind-398223')\n",
    "model_path = \"gs://meter-sam/model/model.pth\"\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "model = SamModel(config=model_config)\n",
    "\n",
    "# with fs.open(model_path, 'rb') as f:\n",
    "#     model_state_dict = torch.load(f)\n",
    "#     model.load_state_dict(model_state_dict)\n",
    "\n",
    "# If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n",
    "with fs.open(model_path, 'rb') as f:\n",
    "    model_state_dict = torch.load(f, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "app = FastAPI(title=\"SAM Vertex AI\")\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    predictions: List[Dict]\n",
    "    \n",
    "@app.get(AIP_HEALTH_ROUTE, status_code=200)\n",
    "async def health():\n",
    "    return {'health': 'ok'}\n",
    "\n",
    "@app.post(AIP_PREDICT_ROUTE, \n",
    "          response_model=Prediction,\n",
    "          response_model_exclude_unset=True)\n",
    "\n",
    "async def predict(request: Request):\n",
    "    try:\n",
    "        body = await request.json()\n",
    "        print(f\"Received request body: {body}\")\n",
    "        instances = body.get('instances')\n",
    "        print(f\"Received request instances: {instances}\")\n",
    "        image_str = [x['image'] for x in body['instances']][0]\n",
    "        print(f\"Received request image: {image_str}\")\n",
    "        \n",
    "        if not image_str:\n",
    "            error_msg = \"Key 'image' not found in the request body.\"\n",
    "            logger.warning(error_msg)\n",
    "            return JSONResponse(content={\"error\": error_msg}, status_code=400)\n",
    "        \n",
    "        # Decode the base64 string and convert the bytes back to an image using PIL\n",
    "        image = Image.open(BytesIO(base64.b64decode(image_str)))\n",
    "        array = np.array(image)\n",
    "        print(f\"array: {array}\")\n",
    "\n",
    "        # Define the size of the array\n",
    "        array_size = array.shape[1]\n",
    "\n",
    "        # Define the size of the grid\n",
    "        grid_size = 10\n",
    "\n",
    "        # Generate the grid points\n",
    "        x = np.linspace(0, array_size-1, grid_size)\n",
    "        y = np.linspace(0, array_size-1, grid_size)\n",
    "\n",
    "        # Generate a grid of coordinates\n",
    "        xv, yv = np.meshgrid(x, y)\n",
    "\n",
    "        # Convert the numpy arrays to lists\n",
    "        xv_list = xv.tolist()\n",
    "        yv_list = yv.tolist()\n",
    "\n",
    "        # Combine the x and y coordinates into a list of list of lists\n",
    "        input_points = [[[int(x), int(y)] for x, y in zip(x_row, y_row)] for x_row, y_row in zip(xv_list, yv_list)]\n",
    "\n",
    "        # Reshape the grid to the expected shape of the input_points tensor\n",
    "        input_points = torch.tensor(input_points).view(1, 1, grid_size*grid_size, 2)\n",
    "\n",
    "        # Process the image\n",
    "        patch = Image.fromarray(array)\n",
    "        inputs = processor(patch, input_points=input_points, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, multimask_output=False)\n",
    "\n",
    "        # Apply sigmoid\n",
    "        patch_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "        patch_prob = patch_prob.cpu().numpy().squeeze()\n",
    "        patch_predict = (patch_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Convert predictions to bytes and then to base64\n",
    "        base64_patch_prob = base64.b64encode(patch_prob.astype(np.float32).tobytes()).decode('utf-8')\n",
    "        base64_patch_predict = base64.b64encode(patch_predict.astype(np.float32).tobytes()).decode('utf-8')\n",
    "        print(f\"base64_patch_prob: {base64_patch_prob}\")\n",
    "        print(f\"base64_patch_predict: {base64_patch_predict}\")\n",
    "        \n",
    "        result={\n",
    "            'mask_probability': base64_patch_prob,\n",
    "            'mask_prediction': base64_patch_predict\n",
    "        }\n",
    "        print(f\"result: {result}\")\n",
    "        \n",
    "        return Prediction(predictions=[result])\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        logger.warning(f\"Value error: {ve}\")\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_docker_file:training,tf-dlvm",
    "tags": []
   },
   "source": [
    "### Write the Dockerfile content\n",
    "\n",
    "Your first step in containerizing your code is to create Dockerfile. In the Dockerfile, you include all the commands needed to run your container image. During the build process, all your packages are installed and an entry point is set for your training and serving code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Dokerfile for trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "write_docker_file:training,tf-dlvm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/train/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/train/Dockerfile\n",
    "# Use the specified PyTorch GPU base image\n",
    "FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-13\n",
    "\n",
    "# Set working directory in the container\n",
    "WORKDIR /train\n",
    "\n",
    "# Install required libraries\n",
    "COPY requirements.txt /train/\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy the trainer code to the docker image\n",
    "COPY trainer.py /train/\n",
    "\n",
    "# Set up the entry point to invoke the trainer\n",
    "ENTRYPOINT [\"python\", \"trainer.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the Dokerfile for Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "write_docker_file:training,tf-dlvm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/serve/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/serve/Dockerfile\n",
    "# Use the specified PyTorch GPU base image\n",
    "FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-13\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /serve\n",
    "\n",
    "# Install required libraries\n",
    "COPY requirements.txt /serve/\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy the server code to the docker image\n",
    "COPY server.py /serve/\n",
    "\n",
    "# Update and install system-level dependencies if needed (like gcc, g++, etc. for certain packages)\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    gcc \\\n",
    "    g++ \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Run server.py when the container launches\n",
    "CMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_enable_api",
    "tags": []
   },
   "source": [
    "## Enable Artifact Registry API\n",
    "\n",
    "You must enable the Artifact Registry API service for your project.\n",
    "\n",
    "Learn more about [Enabling service](https://cloud.google.com/artifact-registry/docs/enable-service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gar_enable_api"
   },
   "outputs": [],
   "source": [
    "! gcloud services enable artifactregistry.googleapis.com\n",
    "\n",
    "if os.getenv(\"IS_TESTING\"):\n",
    "    ! sudo apt-get update --yes && sudo apt-get --only-upgrade --yes install google-cloud-sdk-cloud-run-proxy google-cloud-sdk-harbourbridge google-cloud-sdk-cbt google-cloud-sdk-gke-gcloud-auth-plugin google-cloud-sdk-kpt google-cloud-sdk-local-extract google-cloud-sdk-minikube google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-go google-cloud-sdk-app-engine-python google-cloud-sdk-spanner-emulator google-cloud-sdk-bigtable-emulator google-cloud-sdk-nomos google-cloud-sdk-package-go-module google-cloud-sdk-firestore-emulator kubectl google-cloud-sdk-datastore-emulator google-cloud-sdk-app-engine-python-extras google-cloud-sdk-cloud-build-local google-cloud-sdk-kubectl-oidc google-cloud-sdk-anthos-auth google-cloud-sdk-app-engine-grpc google-cloud-sdk-pubsub-emulator google-cloud-sdk-datalab google-cloud-sdk-skaffold google-cloud-sdk google-cloud-sdk-terraform-tools google-cloud-sdk-config-connector\n",
    "    ! gcloud components update --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gar_create_repo",
    "tags": []
   },
   "source": [
    "## Create a private Docker repository\n",
    "\n",
    "Your first step is to create your own Docker repository in Google Artifact Registry.\n",
    "\n",
    "1. Run the `gcloud artifacts repositories create` command to create a new Docker repository with your region with the description \"docker repository\".\n",
    "\n",
    "2. Run the `gcloud artifacts repositories list` command to verify that your repository was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "gar_create_repo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
      "Listing items under project imposing-mind-398223, across all locations.\n",
      "\n",
      "                                                                   ARTIFACT_REGISTRY\n",
      "REPOSITORY  FORMAT  MODE                 DESCRIPTION        LOCATION     LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
      "meter-sam   DOCKER  STANDARD_REPOSITORY  Docker repository  us-central1          Google-managed key  2023-09-16T18:04:00  2023-10-13T17:21:06  14939.828\n"
     ]
    }
   ],
   "source": [
    "REPOSITORY = \"meter-sam\"\n",
    "\n",
    "! gcloud artifacts repositories create {REPOSITORY} --repository-format=docker --location={REGION} --description=\"Docker repository\"\n",
    "\n",
    "! gcloud artifacts repositories list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "name_container:training",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Build the training container\n",
    "\n",
    "### Create a repository\n",
    "\n",
    "Next, you create a repository in the Artifact Registry where you store your training image.\n",
    "\n",
    "Set names for your repository and custom container below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "name_container:training"
   },
   "outputs": [],
   "source": [
    "CONTAINER_NAME = \"landfill-test-train\"\n",
    "\n",
    "TAG = \"latest\"\n",
    "TRAIN_IMAGE = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{CONTAINER_NAME}:{TAG}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-train:latest'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "build_container:training"
   },
   "source": [
    "Create the repository in Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "register_container:training",
    "tags": []
   },
   "source": [
    "### Build the custom container for training\n",
    "\n",
    "Next, you build and push a docker image to the created repository using Cloud Build. \n",
    "\n",
    "Learn more about the process of [Building and pushing a Docker image with Cloud Build](https://cloud.google.com/build/docs/build-push-docker-image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2e7b3bcc53df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/meter-sam/custom/train\n",
      "Creating temporary tarball archive of 4 file(s) totalling 7.7 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://imposing-mind-398223_cloudbuild/source/1696897867.942236-14965ac74fde4f04b91c17a060563f88.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/imposing-mind-398223/locations/us-central1/builds/a8b53f58-8b18-4dca-a16b-c72aba23df55].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/a8b53f58-8b18-4dca-a16b-c72aba23df55?project=78123506305 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"a8b53f58-8b18-4dca-a16b-c72aba23df55\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://imposing-mind-398223_cloudbuild/source/1696897867.942236-14965ac74fde4f04b91c17a060563f88.tgz#1696897868208207\n",
      "Copying gs://imposing-mind-398223_cloudbuild/source/1696897867.942236-14965ac74fde4f04b91c17a060563f88.tgz#1696897868208207...\n",
      "/ [1 files][  3.2 KiB/  3.2 KiB]                                                \n",
      "Operation completed over 1 objects/3.2 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/6 : FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-13\n",
      "latest: Pulling from deeplearning-platform-release/pytorch-gpu.1-13\n",
      "56e0351b9876: Pulling fs layer\n",
      "18e5fdd4cb87: Pulling fs layer\n",
      "645133b03941: Pulling fs layer\n",
      "b4a1420ffd93: Pulling fs layer\n",
      "7e1f4dad10e9: Pulling fs layer\n",
      "552678304786: Pulling fs layer\n",
      "da8df3f1d840: Pulling fs layer\n",
      "46a89842b228: Pulling fs layer\n",
      "2effcdc05756: Pulling fs layer\n",
      "92f8b08ca694: Pulling fs layer\n",
      "d8e42cb29222: Pulling fs layer\n",
      "d06f83c5e84f: Pulling fs layer\n",
      "f29ad55a6c2e: Pulling fs layer\n",
      "022644608d3e: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ddaa942f8f76: Pulling fs layer\n",
      "0e528b6997d2: Pulling fs layer\n",
      "faa6f939478f: Pulling fs layer\n",
      "cd9415e996a6: Pulling fs layer\n",
      "43cbd8ff4fe4: Pulling fs layer\n",
      "d898aab04ad1: Pulling fs layer\n",
      "d69a35ea6607: Pulling fs layer\n",
      "e3d8031bd108: Pulling fs layer\n",
      "c2ca07f610bb: Pulling fs layer\n",
      "9464d84903dc: Pulling fs layer\n",
      "2ddd62e44342: Pulling fs layer\n",
      "676ff5fae536: Pulling fs layer\n",
      "56657ba70275: Pulling fs layer\n",
      "c299e09a2028: Pulling fs layer\n",
      "aef8715940a2: Pulling fs layer\n",
      "d8d15e1629e3: Pulling fs layer\n",
      "545830d30c07: Pulling fs layer\n",
      "eb1d5fa395f6: Pulling fs layer\n",
      "18f4f93fc4d5: Pulling fs layer\n",
      "a524b3e2624a: Pulling fs layer\n",
      "43cbd8ff4fe4: Waiting\n",
      "b4a1420ffd93: Waiting\n",
      "7e1f4dad10e9: Waiting\n",
      "552678304786: Waiting\n",
      "da8df3f1d840: Waiting\n",
      "46a89842b228: Waiting\n",
      "2effcdc05756: Waiting\n",
      "92f8b08ca694: Waiting\n",
      "d8e42cb29222: Waiting\n",
      "d06f83c5e84f: Waiting\n",
      "f29ad55a6c2e: Waiting\n",
      "022644608d3e: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "ddaa942f8f76: Waiting\n",
      "0e528b6997d2: Waiting\n",
      "faa6f939478f: Waiting\n",
      "cd9415e996a6: Waiting\n",
      "d898aab04ad1: Waiting\n",
      "d69a35ea6607: Waiting\n",
      "e3d8031bd108: Waiting\n",
      "c2ca07f610bb: Waiting\n",
      "9464d84903dc: Waiting\n",
      "2ddd62e44342: Waiting\n",
      "676ff5fae536: Waiting\n",
      "56657ba70275: Waiting\n",
      "c299e09a2028: Waiting\n",
      "aef8715940a2: Waiting\n",
      "d8d15e1629e3: Waiting\n",
      "545830d30c07: Waiting\n",
      "eb1d5fa395f6: Waiting\n",
      "18f4f93fc4d5: Waiting\n",
      "a524b3e2624a: Waiting\n",
      "18e5fdd4cb87: Verifying Checksum\n",
      "18e5fdd4cb87: Download complete\n",
      "645133b03941: Verifying Checksum\n",
      "645133b03941: Download complete\n",
      "b4a1420ffd93: Verifying Checksum\n",
      "b4a1420ffd93: Download complete\n",
      "7e1f4dad10e9: Verifying Checksum\n",
      "7e1f4dad10e9: Download complete\n",
      "da8df3f1d840: Verifying Checksum\n",
      "da8df3f1d840: Download complete\n",
      "56e0351b9876: Verifying Checksum\n",
      "56e0351b9876: Download complete\n",
      "46a89842b228: Verifying Checksum\n",
      "46a89842b228: Download complete\n",
      "2effcdc05756: Verifying Checksum\n",
      "2effcdc05756: Download complete\n",
      "d8e42cb29222: Verifying Checksum\n",
      "d8e42cb29222: Download complete\n",
      "56e0351b9876: Pull complete\n",
      "18e5fdd4cb87: Pull complete\n",
      "552678304786: Verifying Checksum\n",
      "552678304786: Download complete\n",
      "645133b03941: Pull complete\n",
      "b4a1420ffd93: Pull complete\n",
      "7e1f4dad10e9: Pull complete\n",
      "f29ad55a6c2e: Verifying Checksum\n",
      "f29ad55a6c2e: Download complete\n",
      "022644608d3e: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "92f8b08ca694: Verifying Checksum\n",
      "92f8b08ca694: Download complete\n",
      "ddaa942f8f76: Verifying Checksum\n",
      "ddaa942f8f76: Download complete\n",
      "faa6f939478f: Verifying Checksum\n",
      "faa6f939478f: Download complete\n",
      "cd9415e996a6: Verifying Checksum\n",
      "cd9415e996a6: Download complete\n",
      "0e528b6997d2: Verifying Checksum\n",
      "0e528b6997d2: Download complete\n",
      "43cbd8ff4fe4: Verifying Checksum\n",
      "43cbd8ff4fe4: Download complete\n",
      "d898aab04ad1: Verifying Checksum\n",
      "d898aab04ad1: Download complete\n",
      "d69a35ea6607: Verifying Checksum\n",
      "d69a35ea6607: Download complete\n",
      "c2ca07f610bb: Verifying Checksum\n",
      "c2ca07f610bb: Download complete\n",
      "9464d84903dc: Verifying Checksum\n",
      "9464d84903dc: Download complete\n",
      "d06f83c5e84f: Verifying Checksum\n",
      "d06f83c5e84f: Download complete\n",
      "2ddd62e44342: Verifying Checksum\n",
      "2ddd62e44342: Download complete\n",
      "676ff5fae536: Verifying Checksum\n",
      "676ff5fae536: Download complete\n",
      "56657ba70275: Verifying Checksum\n",
      "56657ba70275: Download complete\n",
      "c299e09a2028: Verifying Checksum\n",
      "c299e09a2028: Download complete\n",
      "aef8715940a2: Verifying Checksum\n",
      "aef8715940a2: Download complete\n",
      "545830d30c07: Verifying Checksum\n",
      "545830d30c07: Download complete\n",
      "d8d15e1629e3: Verifying Checksum\n",
      "d8d15e1629e3: Download complete\n",
      "eb1d5fa395f6: Verifying Checksum\n",
      "eb1d5fa395f6: Download complete\n",
      "e3d8031bd108: Verifying Checksum\n",
      "e3d8031bd108: Download complete\n",
      "a524b3e2624a: Download complete\n",
      "552678304786: Pull complete\n",
      "da8df3f1d840: Pull complete\n",
      "46a89842b228: Pull complete\n",
      "2effcdc05756: Pull complete\n",
      "18f4f93fc4d5: Verifying Checksum\n",
      "18f4f93fc4d5: Download complete\n",
      "92f8b08ca694: Pull complete\n",
      "d8e42cb29222: Pull complete\n",
      "d06f83c5e84f: Pull complete\n",
      "f29ad55a6c2e: Pull complete\n",
      "022644608d3e: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ddaa942f8f76: Pull complete\n",
      "0e528b6997d2: Pull complete\n",
      "faa6f939478f: Pull complete\n",
      "cd9415e996a6: Pull complete\n",
      "43cbd8ff4fe4: Pull complete\n",
      "d898aab04ad1: Pull complete\n",
      "d69a35ea6607: Pull complete\n",
      "e3d8031bd108: Pull complete\n",
      "c2ca07f610bb: Pull complete\n",
      "9464d84903dc: Pull complete\n",
      "2ddd62e44342: Pull complete\n",
      "676ff5fae536: Pull complete\n",
      "56657ba70275: Pull complete\n",
      "c299e09a2028: Pull complete\n",
      "aef8715940a2: Pull complete\n",
      "d8d15e1629e3: Pull complete\n",
      "545830d30c07: Pull complete\n",
      "eb1d5fa395f6: Pull complete\n",
      "18f4f93fc4d5: Pull complete\n",
      "a524b3e2624a: Pull complete\n",
      "Digest: sha256:2fe2277f7daaf7b8c7bcb61dee5dc7ea90987151e7a480f82e99536bb5740eb6\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:latest\n",
      " ---> e003452b7a72\n",
      "Step 2/6 : WORKDIR /train\n",
      " ---> Running in 6f260923fee4\n",
      "Removing intermediate container 6f260923fee4\n",
      " ---> 6d16c968e909\n",
      "Step 3/6 : COPY requirements.txt /train/\n",
      " ---> 210fb231be7a\n",
      "Step 4/6 : RUN pip install -r requirements.txt\n",
      " ---> Running in 36275cacc95a\n",
      "Collecting git+https://github.com/facebookresearch/segment-anything.git (from -r requirements.txt (line 1))\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-4f6fz952\n",
      "\u001b[91m  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-4f6fz952\n",
      "\u001b[0m  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.6/113.6 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting datasets (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/d3/95/ef83542e7a8e2bfc4432ee2cd8a6b52eb30fb1e605871e8871e94ce65fb1/datasets-2.13.2-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.13.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting monai (from -r requirements.txt (line 4))\n",
      "  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 28.8 MB/s eta 0:00:00\n",
      "Collecting rasterio (from -r requirements.txt (line 5))\n",
      "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/19.3 MB 62.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/ec/2d/40500c0d370a0633dfc7da82cc15f34bb066af6a485c9e1bc29174c84578/regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 6.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 84.4 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 2))\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/7f/b9/eaa33791dedb2a92a15b6ee36c854f1e9b80caad831fa845e20863a6fee6/safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (4.63.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 2)) (4.11.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (12.0.1)\n",
      "Collecting dill<0.3.7,>=0.3.0 (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 17.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (1.3.5)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/54/a0/dae1c5dc27601a61897b48a367232c743c760c765d9ab38be1a903cf0d87/xxhash-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading xxhash-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 3))\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/ca/3f/8354ce12fd13bd5c5bb4722261a10ca1d6e2eb7c1c08fa3d8a4e9dc98f44/multiprocess-0.70.15-py37-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py37-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 3)) (3.8.5)\n",
      "Requirement already satisfied: torch>=1.8 in /opt/conda/lib/python3.7/site-packages (from monai->-r requirements.txt (line 4)) (1.13.1)\n",
      "Collecting affine (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Collecting snuggs>=1.4.1 (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting click-plugins (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (68.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers->-r requirements.txt (line 2)) (1.26.16)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.8->monai->-r requirements.txt (line 4)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch>=1.8->monai->-r requirements.txt (line 4)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch>=1.8->monai->-r requirements.txt (line 4)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch>=1.8->monai->-r requirements.txt (line 4)) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8->monai->-r requirements.txt (line 4)) (0.41.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->-r requirements.txt (line 2)) (3.15.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets->-r requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.7/115.7 kB 18.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->-r requirements.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets->-r requirements.txt (line 3)) (1.16.0)\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 66.7 MB/s eta 0:00:00\n",
      "Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 512.7/512.7 kB 48.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 34.3 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 761.6/761.6 kB 57.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 77.9 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.6/194.6 kB 29.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: segment-anything\n",
      "  Building wheel for segment-anything (setup.py): started\n",
      "  Building wheel for segment-anything (setup.py): finished with status 'done'\n",
      "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36586 sha256=c5662f59492b7ae51a97d43e62f62daf15902a303e1e6a96da5bff6a9d3ddd41\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3z5d1nxj/wheels/6e/b2/38/082465c177a067501384fd15cb18be1128372ef60f27a1c426\n",
      "Successfully built segment-anything\n",
      "Installing collected packages: tokenizers, segment-anything, xxhash, snuggs, safetensors, regex, dill, affine, multiprocess, huggingface-hub, transformers, cligj, click-plugins, rasterio, monai, datasets\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 datasets-2.13.2 dill-0.3.6 huggingface-hub-0.16.4 monai-1.1.0 multiprocess-0.70.14 rasterio-1.2.10 regex-2023.10.3 safetensors-0.4.0 segment-anything-1.0 snuggs-1.4.7 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.4.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 36275cacc95a\n",
      " ---> 22ada91e03f6\n",
      "Step 5/6 : COPY trainer.py /train/\n",
      " ---> 1231071cb8c2\n",
      "Step 6/6 : ENTRYPOINT [\"python\", \"trainer.py\"]\n",
      " ---> Running in b240496c772a\n",
      "Removing intermediate container b240496c772a\n",
      " ---> 84fb701ac207\n",
      "Successfully built 84fb701ac207\n",
      "Successfully tagged us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-train:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-train:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-train]\n",
      "30e6d430e453: Preparing\n",
      "00e6844e5822: Preparing\n",
      "ba67a0ec1f22: Preparing\n",
      "c259425e56f3: Preparing\n",
      "e6f64de962dc: Preparing\n",
      "63f69a629442: Preparing\n",
      "d9e2b0241882: Preparing\n",
      "f305c3a19090: Preparing\n",
      "9fd8e5a64ccf: Preparing\n",
      "90fbb1ea45fe: Preparing\n",
      "c6b5aab144d8: Preparing\n",
      "5f780a21443c: Preparing\n",
      "f11fc3b84a9f: Preparing\n",
      "320a8f11c8be: Preparing\n",
      "0daba76b686e: Preparing\n",
      "f6cd5d477c3f: Preparing\n",
      "ae655a59ef62: Preparing\n",
      "cf4fe5bb1a64: Preparing\n",
      "fa2b875aafbd: Preparing\n",
      "4446016a6563: Preparing\n",
      "eb91bddea38c: Preparing\n",
      "141f003efb3f: Preparing\n",
      "05a5747fb507: Preparing\n",
      "ef7a45d9868d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7d6bfde9fbe7: Preparing\n",
      "c438e19a23b9: Preparing\n",
      "1e0fad0ce3ec: Preparing\n",
      "9a9d17453a10: Preparing\n",
      "9b60e1a1cbfb: Preparing\n",
      "143c7090d6d6: Preparing\n",
      "09001fa7f0e3: Preparing\n",
      "6ffbef4e19ae: Preparing\n",
      "bb9ed9356ee8: Preparing\n",
      "0fe166373eb9: Preparing\n",
      "2ed07a859620: Preparing\n",
      "995f183bef21: Preparing\n",
      "1e4369562af2: Preparing\n",
      "ec66d8cea54a: Preparing\n",
      "63f69a629442: Waiting\n",
      "d9e2b0241882: Waiting\n",
      "f305c3a19090: Waiting\n",
      "9fd8e5a64ccf: Waiting\n",
      "90fbb1ea45fe: Waiting\n",
      "c6b5aab144d8: Waiting\n",
      "5f780a21443c: Waiting\n",
      "f11fc3b84a9f: Waiting\n",
      "320a8f11c8be: Waiting\n",
      "0daba76b686e: Waiting\n",
      "f6cd5d477c3f: Waiting\n",
      "ae655a59ef62: Waiting\n",
      "cf4fe5bb1a64: Waiting\n",
      "fa2b875aafbd: Waiting\n",
      "4446016a6563: Waiting\n",
      "eb91bddea38c: Waiting\n",
      "141f003efb3f: Waiting\n",
      "05a5747fb507: Waiting\n",
      "ef7a45d9868d: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "7d6bfde9fbe7: Waiting\n",
      "c438e19a23b9: Waiting\n",
      "1e0fad0ce3ec: Waiting\n",
      "9a9d17453a10: Waiting\n",
      "9b60e1a1cbfb: Waiting\n",
      "143c7090d6d6: Waiting\n",
      "09001fa7f0e3: Waiting\n",
      "6ffbef4e19ae: Waiting\n",
      "bb9ed9356ee8: Waiting\n",
      "0fe166373eb9: Waiting\n",
      "2ed07a859620: Waiting\n",
      "995f183bef21: Waiting\n",
      "1e4369562af2: Waiting\n",
      "ec66d8cea54a: Waiting\n",
      "e6f64de962dc: Layer already exists\n",
      "63f69a629442: Layer already exists\n",
      "d9e2b0241882: Layer already exists\n",
      "f305c3a19090: Layer already exists\n",
      "ba67a0ec1f22: Pushed\n",
      "30e6d430e453: Pushed\n",
      "9fd8e5a64ccf: Layer already exists\n",
      "c259425e56f3: Pushed\n",
      "c6b5aab144d8: Layer already exists\n",
      "90fbb1ea45fe: Layer already exists\n",
      "f11fc3b84a9f: Layer already exists\n",
      "5f780a21443c: Layer already exists\n",
      "320a8f11c8be: Layer already exists\n",
      "0daba76b686e: Layer already exists\n",
      "ae655a59ef62: Layer already exists\n",
      "f6cd5d477c3f: Layer already exists\n",
      "cf4fe5bb1a64: Layer already exists\n",
      "4446016a6563: Layer already exists\n",
      "fa2b875aafbd: Layer already exists\n",
      "eb91bddea38c: Layer already exists\n",
      "141f003efb3f: Layer already exists\n",
      "ef7a45d9868d: Layer already exists\n",
      "05a5747fb507: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "1e0fad0ce3ec: Layer already exists\n",
      "7d6bfde9fbe7: Layer already exists\n",
      "c438e19a23b9: Layer already exists\n",
      "9a9d17453a10: Layer already exists\n",
      "6ffbef4e19ae: Layer already exists\n",
      "9b60e1a1cbfb: Layer already exists\n",
      "09001fa7f0e3: Layer already exists\n",
      "143c7090d6d6: Layer already exists\n",
      "995f183bef21: Layer already exists\n",
      "2ed07a859620: Layer already exists\n",
      "bb9ed9356ee8: Layer already exists\n",
      "0fe166373eb9: Layer already exists\n",
      "1e4369562af2: Layer already exists\n",
      "ec66d8cea54a: Pushed\n",
      "00e6844e5822: Pushed\n",
      "latest: digest: sha256:c11573e5095e8d8647cd3dd7b4d4cba6660e720740df5d1dc62ac7aceee2fd70 size: 8492\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                              IMAGES                                                                                   STATUS\n",
      "a8b53f58-8b18-4dca-a16b-c72aba23df55  2023-10-10T00:31:08+00:00  7M7S      gs://imposing-mind-398223_cloudbuild/source/1696897867.942236-14965ac74fde4f04b91c17a060563f88.tgz  us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-train (+1 more)  SUCCESS\n",
      "/home/jupyter/meter-sam/custom\n"
     ]
    }
   ],
   "source": [
    "%cd custom/train\n",
    "!gcloud builds submit --quiet --region={REGION} --tag=$TRAIN_IMAGE\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_training_job:mbsdk,no_model,custom",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "For more details, see [Custom containers overview]([training.containers-overview](https://cloud.google.com/vertex-ai/docs/training/containers-overview).\n",
    "\n",
    "### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The training container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "create_custom_training_job:mbsdk,no_model,custom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.CustomContainerTrainingJob object at 0x7f7ca23ea790>\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"test-35000-base\", container_uri=TRAIN_IMAGE\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model",
    "tags": []
   },
   "source": [
    "### Run the custom training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `args`: The command-line arguments to pass to the training script.\n",
    "- `replica_count`: The number of compute instances for training (replica_count = 1 is single node training).\n",
    "- `machine_type`: The machine type for the compute instances.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `base_output_dir`: The Cloud Storage location to write the model artifacts to.\n",
    "- `sync`: Whether to block until completion of the job."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "--input-dir=gs://meter-sam/test/stack/\n",
    "--output-dir=gs://meter-sam/test/model/\n",
    "--num-epochs=1\n",
    "--batch-size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://meter-sam/test/model/ \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7748952576371982336?project=78123506305\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training failed with:\ncode: 8\nmessage: \"The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_nvidia_t4_gpus\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/3919624199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_GPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0maccelerator_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_NGPU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries)\u001b[0m\n\u001b[1;32m   4662\u001b[0m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4663\u001b[0m             \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4664\u001b[0;31m             \u001b[0mdisable_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4665\u001b[0m         )\n\u001b[1;32m   4666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout, block, disable_retries)\u001b[0m\n\u001b[1;32m   5356\u001b[0m             \u001b[0mbigquery_destination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigquery_destination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5357\u001b[0m             \u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5358\u001b[0;31m             \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5359\u001b[0m         )\n\u001b[1;32m   5360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout, block)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"View Training:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dashboard_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_get_model\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \"\"\"\n\u001b[1;32m    917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_failed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_JOB_WAIT_TIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_completed_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/training_jobs.py\u001b[0m in \u001b[0;36m_raise_failure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcode_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training failed with:\\n%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training failed with:\ncode: 8\nmessage: \"The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_nvidia_t4_gpus\"\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = \"gs://meter-sam/test/stack/\"\n",
    "OUTPUT_DIR = \"gs://meter-sam/test/model/\"\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--input-dir=\" + INPUT_DIR,\n",
    "    \"--output-dir=\" + OUTPUT_DIR,\n",
    "    \"--num-epochs=\" + str(EPOCHS),\n",
    "    \"--batch-size=\" + str(BATCH_SIZE),\n",
    "]\n",
    "\n",
    "\n",
    "# MODEL_DIR = \"{}/{}\".format(BUCKET_URI, UUID)\n",
    "# DIRECT = True\n",
    "# if DIRECT:\n",
    "#     CMDARGS = [\n",
    "#         \"--model-dir=\" + MODEL_DIR,\n",
    "#         \"--epochs=\" + str(EPOCHS),\n",
    "#         \"--steps=\" + str(STEPS),\n",
    "#     ]\n",
    "# else:\n",
    "#     CMDARGS = [\n",
    "#         \"--epochs=\" + str(EPOCHS),\n",
    "#         \"--steps=\" + str(STEPS),\n",
    "#     ]\n",
    "\n",
    "if TRAIN_GPU:\n",
    "    job.run(\n",
    "        base_output_dir=OUTPUT_DIR,\n",
    "        service_account='78123506305-compute@developer.gserviceaccount.com',\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        accelerator_type=TRAIN_GPU.name,\n",
    "        accelerator_count=TRAIN_NGPU,\n",
    "        sync=True,\n",
    "    )\n",
    "else:\n",
    "    job.run(\n",
    "        base_output_dir=OUTPUT_DIR,\n",
    "        service_account='78123506305-compute@developer.gserviceaccount.com',\n",
    "        args=CMDARGS,\n",
    "        replica_count=1,\n",
    "        machine_type=TRAIN_COMPUTE,\n",
    "        sync=False,\n",
    "    )\n",
    "\n",
    "model_path_to_deploy = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_job:mbsdk,no_model",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wait for completion of custom training job\n",
    "\n",
    "Next, wait for the custom training job to complete. Alternatively, one can set the parameter `sync` to `True` in the `run()` method to block until the custom training job is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "name_container:training",
    "tags": []
   },
   "source": [
    "## Build the serving container\n",
    "\n",
    "### Create a repository\n",
    "\n",
    "Next, you create a repository in the Artifact Registry where you store your training image.\n",
    "\n",
    "Set names for your repository and custom container below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "name_container:training"
   },
   "outputs": [],
   "source": [
    "CONTAINER_NAME = \"landfill-test-serve\"\n",
    "\n",
    "TAG = \"latest\"\n",
    "SERVER_IMAGE = (\n",
    "    f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{CONTAINER_NAME}:{TAG}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-serve:latest'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVER_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "build_container:training"
   },
   "source": [
    "Create the repository in Artifact Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "register_container:training",
    "tags": []
   },
   "source": [
    "### Build the custom container for serving\n",
    "\n",
    "Next, you build and push a docker image to the created repository using Cloud Build. \n",
    "\n",
    "Learn more about the process of [Building and pushing a Docker image with Cloud Build](https://cloud.google.com/build/docs/build-push-docker-image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2e7b3bcc53df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/custom/serve\n",
      "Creating temporary tarball archive of 4 file(s) totalling 5.5 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://imposing-mind-398223_cloudbuild/source/1697563778.821047-e9ae6d97c7cc4dcd8973d5ddbb5afee4.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/imposing-mind-398223/locations/us-central1/builds/6223885e-8185-4271-bc88-a8efa34941e1].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/6223885e-8185-4271-bc88-a8efa34941e1?project=78123506305 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6223885e-8185-4271-bc88-a8efa34941e1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://imposing-mind-398223_cloudbuild/source/1697563778.821047-e9ae6d97c7cc4dcd8973d5ddbb5afee4.tgz#1697563779027702\n",
      "Copying gs://imposing-mind-398223_cloudbuild/source/1697563778.821047-e9ae6d97c7cc4dcd8973d5ddbb5afee4.tgz#1697563779027702...\n",
      "/ [1 files][  2.6 KiB/  2.6 KiB]                                                \n",
      "Operation completed over 1 objects/2.6 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/pytorch-gpu.1-13\n",
      "latest: Pulling from deeplearning-platform-release/pytorch-gpu.1-13\n",
      "56e0351b9876: Pulling fs layer\n",
      "18e5fdd4cb87: Pulling fs layer\n",
      "645133b03941: Pulling fs layer\n",
      "b4a1420ffd93: Pulling fs layer\n",
      "7e1f4dad10e9: Pulling fs layer\n",
      "552678304786: Pulling fs layer\n",
      "da8df3f1d840: Pulling fs layer\n",
      "46a89842b228: Pulling fs layer\n",
      "2effcdc05756: Pulling fs layer\n",
      "92f8b08ca694: Pulling fs layer\n",
      "d8e42cb29222: Pulling fs layer\n",
      "d06f83c5e84f: Pulling fs layer\n",
      "f29ad55a6c2e: Pulling fs layer\n",
      "022644608d3e: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "ddaa942f8f76: Pulling fs layer\n",
      "0e528b6997d2: Pulling fs layer\n",
      "faa6f939478f: Pulling fs layer\n",
      "cd9415e996a6: Pulling fs layer\n",
      "43cbd8ff4fe4: Pulling fs layer\n",
      "d898aab04ad1: Pulling fs layer\n",
      "d69a35ea6607: Pulling fs layer\n",
      "e3d8031bd108: Pulling fs layer\n",
      "c2ca07f610bb: Pulling fs layer\n",
      "9464d84903dc: Pulling fs layer\n",
      "2ddd62e44342: Pulling fs layer\n",
      "676ff5fae536: Pulling fs layer\n",
      "56657ba70275: Pulling fs layer\n",
      "c299e09a2028: Pulling fs layer\n",
      "aef8715940a2: Pulling fs layer\n",
      "d8d15e1629e3: Pulling fs layer\n",
      "545830d30c07: Pulling fs layer\n",
      "eb1d5fa395f6: Pulling fs layer\n",
      "18f4f93fc4d5: Pulling fs layer\n",
      "a524b3e2624a: Pulling fs layer\n",
      "b4a1420ffd93: Waiting\n",
      "7e1f4dad10e9: Waiting\n",
      "552678304786: Waiting\n",
      "da8df3f1d840: Waiting\n",
      "46a89842b228: Waiting\n",
      "2effcdc05756: Waiting\n",
      "92f8b08ca694: Waiting\n",
      "d8e42cb29222: Waiting\n",
      "d06f83c5e84f: Waiting\n",
      "f29ad55a6c2e: Waiting\n",
      "022644608d3e: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "ddaa942f8f76: Waiting\n",
      "0e528b6997d2: Waiting\n",
      "faa6f939478f: Waiting\n",
      "cd9415e996a6: Waiting\n",
      "43cbd8ff4fe4: Waiting\n",
      "d898aab04ad1: Waiting\n",
      "d69a35ea6607: Waiting\n",
      "e3d8031bd108: Waiting\n",
      "c2ca07f610bb: Waiting\n",
      "9464d84903dc: Waiting\n",
      "2ddd62e44342: Waiting\n",
      "676ff5fae536: Waiting\n",
      "56657ba70275: Waiting\n",
      "c299e09a2028: Waiting\n",
      "aef8715940a2: Waiting\n",
      "d8d15e1629e3: Waiting\n",
      "545830d30c07: Waiting\n",
      "eb1d5fa395f6: Waiting\n",
      "18f4f93fc4d5: Waiting\n",
      "a524b3e2624a: Waiting\n",
      "18e5fdd4cb87: Verifying Checksum\n",
      "18e5fdd4cb87: Download complete\n",
      "645133b03941: Verifying Checksum\n",
      "645133b03941: Download complete\n",
      "7e1f4dad10e9: Verifying Checksum\n",
      "7e1f4dad10e9: Download complete\n",
      "b4a1420ffd93: Verifying Checksum\n",
      "b4a1420ffd93: Download complete\n",
      "56e0351b9876: Verifying Checksum\n",
      "56e0351b9876: Download complete\n",
      "da8df3f1d840: Download complete\n",
      "46a89842b228: Verifying Checksum\n",
      "46a89842b228: Download complete\n",
      "2effcdc05756: Verifying Checksum\n",
      "2effcdc05756: Download complete\n",
      "d8e42cb29222: Verifying Checksum\n",
      "d8e42cb29222: Download complete\n",
      "56e0351b9876: Pull complete\n",
      "552678304786: Verifying Checksum\n",
      "552678304786: Download complete\n",
      "18e5fdd4cb87: Pull complete\n",
      "f29ad55a6c2e: Verifying Checksum\n",
      "f29ad55a6c2e: Download complete\n",
      "022644608d3e: Verifying Checksum\n",
      "022644608d3e: Download complete\n",
      "645133b03941: Pull complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "92f8b08ca694: Verifying Checksum\n",
      "92f8b08ca694: Download complete\n",
      "b4a1420ffd93: Pull complete\n",
      "7e1f4dad10e9: Pull complete\n",
      "ddaa942f8f76: Verifying Checksum\n",
      "ddaa942f8f76: Download complete\n",
      "0e528b6997d2: Verifying Checksum\n",
      "0e528b6997d2: Download complete\n",
      "cd9415e996a6: Verifying Checksum\n",
      "cd9415e996a6: Download complete\n",
      "43cbd8ff4fe4: Verifying Checksum\n",
      "43cbd8ff4fe4: Download complete\n",
      "faa6f939478f: Verifying Checksum\n",
      "faa6f939478f: Download complete\n",
      "d69a35ea6607: Verifying Checksum\n",
      "d69a35ea6607: Download complete\n",
      "d898aab04ad1: Verifying Checksum\n",
      "d898aab04ad1: Download complete\n",
      "c2ca07f610bb: Verifying Checksum\n",
      "c2ca07f610bb: Download complete\n",
      "9464d84903dc: Verifying Checksum\n",
      "9464d84903dc: Download complete\n",
      "2ddd62e44342: Verifying Checksum\n",
      "2ddd62e44342: Download complete\n",
      "676ff5fae536: Verifying Checksum\n",
      "676ff5fae536: Download complete\n",
      "56657ba70275: Verifying Checksum\n",
      "56657ba70275: Download complete\n",
      "c299e09a2028: Verifying Checksum\n",
      "c299e09a2028: Download complete\n",
      "aef8715940a2: Verifying Checksum\n",
      "aef8715940a2: Download complete\n",
      "d8d15e1629e3: Verifying Checksum\n",
      "d8d15e1629e3: Download complete\n",
      "545830d30c07: Download complete\n",
      "eb1d5fa395f6: Verifying Checksum\n",
      "eb1d5fa395f6: Download complete\n",
      "d06f83c5e84f: Verifying Checksum\n",
      "d06f83c5e84f: Download complete\n",
      "e3d8031bd108: Verifying Checksum\n",
      "e3d8031bd108: Download complete\n",
      "a524b3e2624a: Verifying Checksum\n",
      "a524b3e2624a: Download complete\n",
      "18f4f93fc4d5: Verifying Checksum\n",
      "18f4f93fc4d5: Download complete\n",
      "552678304786: Pull complete\n",
      "da8df3f1d840: Pull complete\n",
      "46a89842b228: Pull complete\n",
      "2effcdc05756: Pull complete\n",
      "92f8b08ca694: Pull complete\n",
      "d8e42cb29222: Pull complete\n",
      "d06f83c5e84f: Pull complete\n",
      "f29ad55a6c2e: Pull complete\n",
      "022644608d3e: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "ddaa942f8f76: Pull complete\n",
      "0e528b6997d2: Pull complete\n",
      "faa6f939478f: Pull complete\n",
      "cd9415e996a6: Pull complete\n",
      "43cbd8ff4fe4: Pull complete\n",
      "d898aab04ad1: Pull complete\n",
      "d69a35ea6607: Pull complete\n",
      "e3d8031bd108: Pull complete\n",
      "c2ca07f610bb: Pull complete\n",
      "9464d84903dc: Pull complete\n",
      "2ddd62e44342: Pull complete\n",
      "676ff5fae536: Pull complete\n",
      "56657ba70275: Pull complete\n",
      "c299e09a2028: Pull complete\n",
      "aef8715940a2: Pull complete\n",
      "d8d15e1629e3: Pull complete\n",
      "545830d30c07: Pull complete\n",
      "eb1d5fa395f6: Pull complete\n",
      "18f4f93fc4d5: Pull complete\n",
      "a524b3e2624a: Pull complete\n",
      "Digest: sha256:2fe2277f7daaf7b8c7bcb61dee5dc7ea90987151e7a480f82e99536bb5740eb6\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:latest\n",
      " ---> e003452b7a72\n",
      "Step 2/7 : WORKDIR /serve\n",
      " ---> Running in 0abeb08d27d5\n",
      "Removing intermediate container 0abeb08d27d5\n",
      " ---> 252b8b68530a\n",
      "Step 3/7 : COPY requirements.txt /serve/\n",
      " ---> 96ec572a9651\n",
      "Step 4/7 : RUN pip install -r requirements.txt\n",
      " ---> Running in 4f951d38d75d\n",
      "Collecting transformers (from -r requirements.txt (line 1))\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113.6/113.6 kB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (9.5.0)\n",
      "Collecting rasterio (from -r requirements.txt (line 5))\n",
      "  Downloading rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.3/19.3 MB 60.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gcsfs in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (2023.1.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (0.103.1)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (0.22.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.14.1 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/ec/2d/40500c0d370a0633dfc7da82cc15f34bb066af6a485c9e1bc29174c84578/regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 78.9 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/7f/b9/eaa33791dedb2a92a15b6ee36c854f1e9b80caad831fa845e20863a6fee6/safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (4.63.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (4.11.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->-r requirements.txt (line 2)) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch->-r requirements.txt (line 2)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch->-r requirements.txt (line 2)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch->-r requirements.txt (line 2)) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (68.2.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->-r requirements.txt (line 2)) (0.41.2)\n",
      "Collecting affine (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio->-r requirements.txt (line 5)) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Collecting snuggs>=1.4.1 (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
      "Collecting click-plugins (from rasterio->-r requirements.txt (line 5))\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from gcsfs->-r requirements.txt (line 6)) (3.8.5)\n",
      "Requirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs->-r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: fsspec==2023.1.0 in /opt/conda/lib/python3.7/site-packages (from gcsfs->-r requirements.txt (line 6)) (2023.1.0)\n",
      "Requirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.7/site-packages (from gcsfs->-r requirements.txt (line 6)) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.7/site-packages (from gcsfs->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (from gcsfs->-r requirements.txt (line 6)) (2.11.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/conda/lib/python3.7/site-packages (from fastapi->-r requirements.txt (line 7)) (3.7.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from fastapi->-r requirements.txt (line 7)) (1.10.12)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.7/site-packages (from fastapi->-r requirements.txt (line 7)) (0.27.0)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn->-r requirements.txt (line 8)) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r requirements.txt (line 6)) (0.13.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.7/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.7/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.7/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 7)) (1.1.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (1.26.16)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib->gcsfs->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->gcsfs->-r requirements.txt (line 6)) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->gcsfs->-r requirements.txt (line 6)) (2.3.3)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage->gcsfs->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers->-r requirements.txt (line 1)) (3.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->-r requirements.txt (line 6)) (1.60.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->gcsfs->-r requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage->gcsfs->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->-r requirements.txt (line 6)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements.txt (line 6)) (3.2.2)\n",
      "Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.2/7.2 MB 86.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 30.4 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 761.6/761.6 kB 52.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 61.7 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, snuggs, safetensors, regex, affine, huggingface-hub, transformers, cligj, click-plugins, rasterio\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 huggingface-hub-0.16.4 rasterio-1.2.10 regex-2023.10.3 safetensors-0.4.0 snuggs-1.4.7 tokenizers-0.13.3 transformers-4.30.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 4f951d38d75d\n",
      " ---> 47d57c68f397\n",
      "Step 5/7 : COPY server.py /serve/\n",
      " ---> a2a4a7c60260\n",
      "Step 6/7 : RUN apt-get update && apt-get install -y     gcc     g++     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 598112a562d5\n",
      "Hit:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease\n",
      "Get:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
      "Get:3 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:9 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [530 kB]\n",
      "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1188 kB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1117 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.0 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3046 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3112 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2896 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.3 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1421 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3600 kB]\n",
      "Fetched 17.3 MB in 2s (9022 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "g++ is already the newest version (4:9.3.0-1ubuntu2).\n",
      "g++ set to manually installed.\n",
      "gcc is already the newest version (4:9.3.0-1ubuntu2).\n",
      "gcc set to manually installed.\n",
      "The following package was automatically installed and is no longer required:\n",
      "  nsight-compute-2021.1.0\n",
      "Use 'apt autoremove' to remove it.\n",
      "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n",
      "Removing intermediate container 598112a562d5\n",
      " ---> 2b03586c0c3d\n",
      "Step 7/7 : CMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n",
      " ---> Running in a557a9926f62\n",
      "Removing intermediate container a557a9926f62\n",
      " ---> 22155a3e331c\n",
      "Successfully built 22155a3e331c\n",
      "Successfully tagged us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-serve:latest\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-serve:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-serve]\n",
      "8187b085f637: Preparing\n",
      "49a6bcdc9298: Preparing\n",
      "dcfd889e2de7: Preparing\n",
      "9e6a2c5a674e: Preparing\n",
      "b6de225efe8d: Preparing\n",
      "e6f64de962dc: Preparing\n",
      "63f69a629442: Preparing\n",
      "d9e2b0241882: Preparing\n",
      "f305c3a19090: Preparing\n",
      "9fd8e5a64ccf: Preparing\n",
      "90fbb1ea45fe: Preparing\n",
      "c6b5aab144d8: Preparing\n",
      "5f780a21443c: Preparing\n",
      "f11fc3b84a9f: Preparing\n",
      "320a8f11c8be: Preparing\n",
      "0daba76b686e: Preparing\n",
      "f6cd5d477c3f: Preparing\n",
      "ae655a59ef62: Preparing\n",
      "cf4fe5bb1a64: Preparing\n",
      "fa2b875aafbd: Preparing\n",
      "4446016a6563: Preparing\n",
      "eb91bddea38c: Preparing\n",
      "141f003efb3f: Preparing\n",
      "05a5747fb507: Preparing\n",
      "ef7a45d9868d: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7d6bfde9fbe7: Preparing\n",
      "c438e19a23b9: Preparing\n",
      "1e0fad0ce3ec: Preparing\n",
      "9a9d17453a10: Preparing\n",
      "9b60e1a1cbfb: Preparing\n",
      "143c7090d6d6: Preparing\n",
      "09001fa7f0e3: Preparing\n",
      "6ffbef4e19ae: Preparing\n",
      "bb9ed9356ee8: Preparing\n",
      "0fe166373eb9: Preparing\n",
      "2ed07a859620: Preparing\n",
      "995f183bef21: Preparing\n",
      "1e4369562af2: Preparing\n",
      "ec66d8cea54a: Preparing\n",
      "e6f64de962dc: Waiting\n",
      "63f69a629442: Waiting\n",
      "d9e2b0241882: Waiting\n",
      "f305c3a19090: Waiting\n",
      "9fd8e5a64ccf: Waiting\n",
      "90fbb1ea45fe: Waiting\n",
      "c6b5aab144d8: Waiting\n",
      "5f780a21443c: Waiting\n",
      "f11fc3b84a9f: Waiting\n",
      "320a8f11c8be: Waiting\n",
      "0daba76b686e: Waiting\n",
      "f6cd5d477c3f: Waiting\n",
      "ae655a59ef62: Waiting\n",
      "cf4fe5bb1a64: Waiting\n",
      "fa2b875aafbd: Waiting\n",
      "4446016a6563: Waiting\n",
      "eb91bddea38c: Waiting\n",
      "141f003efb3f: Waiting\n",
      "05a5747fb507: Waiting\n",
      "ef7a45d9868d: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "7d6bfde9fbe7: Waiting\n",
      "c438e19a23b9: Waiting\n",
      "1e0fad0ce3ec: Waiting\n",
      "9a9d17453a10: Waiting\n",
      "9b60e1a1cbfb: Waiting\n",
      "143c7090d6d6: Waiting\n",
      "09001fa7f0e3: Waiting\n",
      "6ffbef4e19ae: Waiting\n",
      "bb9ed9356ee8: Waiting\n",
      "0fe166373eb9: Waiting\n",
      "2ed07a859620: Waiting\n",
      "995f183bef21: Waiting\n",
      "1e4369562af2: Waiting\n",
      "ec66d8cea54a: Waiting\n",
      "9e6a2c5a674e: Pushed\n",
      "49a6bcdc9298: Pushed\n",
      "b6de225efe8d: Pushed\n",
      "63f69a629442: Layer already exists\n",
      "e6f64de962dc: Layer already exists\n",
      "d9e2b0241882: Layer already exists\n",
      "8187b085f637: Pushed\n",
      "f305c3a19090: Layer already exists\n",
      "90fbb1ea45fe: Layer already exists\n",
      "9fd8e5a64ccf: Layer already exists\n",
      "c6b5aab144d8: Layer already exists\n",
      "5f780a21443c: Layer already exists\n",
      "f6cd5d477c3f: Layer already exists\n",
      "f11fc3b84a9f: Layer already exists\n",
      "0daba76b686e: Layer already exists\n",
      "320a8f11c8be: Layer already exists\n",
      "4446016a6563: Layer already exists\n",
      "fa2b875aafbd: Layer already exists\n",
      "ae655a59ef62: Layer already exists\n",
      "cf4fe5bb1a64: Layer already exists\n",
      "eb91bddea38c: Layer already exists\n",
      "141f003efb3f: Layer already exists\n",
      "05a5747fb507: Layer already exists\n",
      "ef7a45d9868d: Layer already exists\n",
      "7d6bfde9fbe7: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "c438e19a23b9: Layer already exists\n",
      "1e0fad0ce3ec: Layer already exists\n",
      "9b60e1a1cbfb: Layer already exists\n",
      "143c7090d6d6: Layer already exists\n",
      "9a9d17453a10: Layer already exists\n",
      "09001fa7f0e3: Layer already exists\n",
      "0fe166373eb9: Layer already exists\n",
      "bb9ed9356ee8: Layer already exists\n",
      "6ffbef4e19ae: Layer already exists\n",
      "2ed07a859620: Layer already exists\n",
      "995f183bef21: Layer already exists\n",
      "1e4369562af2: Layer already exists\n",
      "ec66d8cea54a: Pushed\n",
      "dcfd889e2de7: Pushed\n",
      "latest: digest: sha256:2579f7c82725aac006f8f09c445c38219f5604cf1b32c96eab531d7482da216b size: 8700\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                              IMAGES                                                                                   STATUS\n",
      "6223885e-8185-4271-bc88-a8efa34941e1  2023-10-17T17:29:39+00:00  5M60S     gs://imposing-mind-398223_cloudbuild/source/1697563778.821047-e9ae6d97c7cc4dcd8973d5ddbb5afee4.tgz  us-central1-docker.pkg.dev/imposing-mind-398223/meter-sam/landfill-test-serve (+1 more)  SUCCESS\n",
      "/home/jupyter/custom\n"
     ]
    }
   ],
   "source": [
    "%cd custom/serve\n",
    "!gcloud builds submit --quiet --region={REGION} --tag=$SERVER_IMAGE\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tarball_training_script",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Store script on your Cloud Storage bucket(optional)\n",
    "\n",
    "Next, you package the serving folder into a compressed tar ball, and then store it in your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tarball_training_script"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: custom: Cannot stat: No such file or directory\n",
      "tar: Exiting with failure status due to previous errors\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][   56.0 B/   56.0 B]                                                \n",
      "Operation completed over 1 objects/56.0 B.                                       \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz gs://meter-sam/trainer/landfill_test.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model:mbsdk",
    "tags": []
   },
   "source": [
    "## Upload the model\n",
    "\n",
    "Next, upload your model to a `Model` resource using `Model.upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Model` resource.\n",
    "- `artifact`: The Cloud Storage location of the trained model artifacts.\n",
    "- `serving_container_image_uri`: The serving container image.\n",
    "- `sync`: Whether to execute the upload asynchronously or synchronously.\n",
    "\n",
    "If the `upload()` method is run asynchronously, you can subsequently block until completion with the `wait()` method.\n",
    "\n",
    "Learn more about [Importing models to Vertex AI](https://cloud.google.com/vertex-ai/docs/general/import-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The exported model directory\n",
    "OUTPUT_DIR = \"gs://meter-sam/test/model/\"\n",
    "\n",
    "# The serving port.\n",
    "SERVE_PORT = 8080\n",
    "\n",
    "# The task name\n",
    "task=\"serving-sam\"\n",
    "\n",
    "# The model ID\n",
    "model_id=\"facebook/sam-vit-base\"\n",
    "\n",
    "# The serving environment\n",
    "serving_env = {\n",
    "    \"MODEL_ID\": model_id,\n",
    "    \"TASK\": task,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    serving_container_image_uri=SERVER_IMAGE,\n",
    "    artifact_uri=OUTPUT_DIR,\n",
    "    serving_container_predict_route=\"/predict\",\n",
    "    serving_container_health_route=\"/health\",\n",
    "    serving_container_ports=[SERVE_PORT],\n",
    "    serving_container_environment_variables=serving_env,\n",
    "    display_name=task,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,all",
    "tags": []
   },
   "source": [
    "## Make online predictions\n",
    "\n",
    "You must deploy a model to an endpoint before that model can be used to serve online predictions. Deploying a model associates physical resources with the model so it can serve online predictions with low latency.\n",
    "\n",
    "For more details, see [Overview of getting predictions on Vertex AI](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api).\n",
    "\n",
    "### Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic is split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/78123506305/locations/us-central1/endpoints/1520055034191020032/operations/3214101667444162560\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/78123506305/locations/us-central1/models/3256619301053923328/operations/2124230557620502528\n",
      "Endpoint created. Resource name: projects/78123506305/locations/us-central1/endpoints/1520055034191020032\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/78123506305/locations/us-central1/endpoints/1520055034191020032')\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"sam-serving\"\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "endpoint = aiplatform.Endpoint.create(display_name=f\"{DEPLOYED_NAME}-endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created. Resource name: projects/78123506305/locations/us-central1/models/3256619301053923328@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/78123506305/locations/us-central1/models/3256619301053923328@1')\n",
      "Deploying model to Endpoint : projects/78123506305/locations/us-central1/endpoints/1520055034191020032\n",
      "Deploy Endpoint model backing LRO: projects/78123506305/locations/us-central1/endpoints/1520055034191020032/operations/445513796518150144\n"
     ]
    }
   ],
   "source": [
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "        accelerator_type=DEPLOY_GPU,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        service_account='78123506305-compute@developer.gserviceaccount.com',\n",
    "        deploy_request_timeout=1800,\n",
    "    )      \n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "        accelerator_type=DEPLOY_GPU,\n",
    "        accelerator_count=0,\n",
    "        service_account='78123506305-compute@developer.gserviceaccount.com',\n",
    "        deploy_request_timeout=1800,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### Get test item\n",
    "\n",
    "You use an example out of the test (holdout) portion of the dataset as a test item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import rasterio\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the GCS path\n",
    "gcs_path = 'gs://meter-sam/test/'\n",
    "file_name='-104.884360899962_38.8362801837095.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get_test_item:test"
   },
   "outputs": [],
   "source": [
    "# Open the file using gcsfs and read the GeoTIFF with rasterio\n",
    "fs = gcsfs.GCSFileSystem(project='imposing-mind-398223')\n",
    "with fs.open(gcs_path+'image/'+file_name, 'rb') as f, rasterio.open(f) as src:\n",
    "    transform = src.transform\n",
    "    crs = src.crs\n",
    "\n",
    "    # Read raster data\n",
    "    array = src.read()\n",
    "    \n",
    "    # Squeeze out the first dimension to get (256, 256)\n",
    "    array = array.squeeze()\n",
    "    \n",
    "    # Convert to PNG\n",
    "    img = Image.fromarray(array.astype(np.uint8))  # Make sure to cast to uint8\n",
    "    buffer = BytesIO()\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "    png_data = buffer.getvalue()\n",
    "\n",
    "# Encode the PNG data to base64 and decode to string\n",
    "image_str = base64.b64encode(png_data).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# response = requests.post('projects/78123506305/locations/us-central1/models/1309111529378938880@1/predict', json=request)\n",
    "# print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "source": [
    "### Prepare the request content\n",
    "#### Request\n",
    "\n",
    "Since in this example your test item is in a Cloud Storage bucket, you open and read the contents of the image using `tf.io.gfile.Gfile()`. To pass the test data to the prediction service, you encode the bytes into base64., which makes the content safe from modification while transmitting binary data over the network.\n",
    "\n",
    "The format of each instance is:\n",
    "\n",
    "    { serving_input: { 'b64': base64_encoded_bytes } }\n",
    "\n",
    "Since the `predict()` method can take multiple items (instances), send your single test item as a list of one test item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare your request data\n",
    "instances = [\n",
    "    {\"image\": image_str}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_test_item:test,image"
   },
   "outputs": [],
   "source": [
    "# instance = {\"image\": image_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "predict_request:mbsdk,custom,icn"
   },
   "source": [
    "### Make the prediction\n",
    "\n",
    "Now that your `Model` resource is deployed to an `Endpoint` resource, you can do online predictions by sending prediction requests to the Endpoint resource.\n",
    "\n",
    "#### Response\n",
    "\n",
    "The response from the `predict()` call is a Python dictionary with the following entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "predict_request:mbsdk,custom,icn"
   },
   "outputs": [],
   "source": [
    "# The format of each instance should conform to the deployed model's prediction input schema.\n",
    "Prediction = endpoint.predict(instances=instances).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instances: Union[Dict, List[Dict]],\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    \"\"\"\n",
    "    `instances` can be either single instance of type dict or a list\n",
    "    of instances.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    instances = instances if isinstance(instances, list) else [instances]\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "    ]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        # print(\" prediction:\", dict(prediction))\n",
    "        return dict(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerenate prediction\n",
    "prediction = predict_custom_trained_model_sample(\n",
    "    project=\"imposing-mind-398223\",\n",
    "    endpoint_id=\"1520055034191020032\",\n",
    "    location=\"us-central1\",\n",
    "    instances=instances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base64_to_np_array(encoded_data):\n",
    "    decoded_data = base64.b64decode(encoded_data)\n",
    "    array_data = np.frombuffer(decoded_data, dtype=np.float32)\n",
    "    return array_data\n",
    "\n",
    "# Decode your results\n",
    "mask_prob_array = base64_to_np_array(prediction[\"mask_probability\"])\n",
    "mask_predict_array = base64_to_np_array(prediction[\"mask_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visulize and compare masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reshape arrays\n",
    "mask_prob_array = mask_prob_array.reshape(256, 256)\n",
    "mask_predict_array = mask_predict_array.reshape(256, 256)\n",
    "\n",
    "# Load the ground truth mask:\n",
    "with fs.open(gcs_path+'mask/'+file_name, 'rb') as f, rasterio.open(f) as src:\n",
    "    ground_truth_array = src.read(1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(18, 6))  # Change to 3 subplots\n",
    "\n",
    "# Display mask probability\n",
    "cax1 = ax[0].imshow(mask_prob_array, cmap='viridis')\n",
    "fig.colorbar(cax1, ax=ax[0])\n",
    "ax[0].set_title('Mask Probability')\n",
    "\n",
    "# Display predicted mask\n",
    "cax2 = ax[1].imshow(mask_predict_array, cmap='gray')\n",
    "ax[1].set_title('Predicted Mask')\n",
    "\n",
    "# Display ground truth mask\n",
    "cax3 = ax[2].imshow(ground_truth_array, cmap='gray')\n",
    "ax[2].set_title('Ground Truth Mask')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = np.sum(ground_truth_array == mask_predict_array)\n",
    "total_predictions = ground_truth_array.size\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Convert and save masks to geotiff files on Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "Attempt to create new tiff file 'satellite_images/predict/probability/-104.884360899962_38.8362801837095.tif' failed: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetWriterBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_err.pyx\u001b[0m in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: Attempt to create new tiff file 'satellite_images/predict/probability/-104.884360899962_38.8362801837095.tif' failed: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1/2545269990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpredict_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'satellite_images/predict/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msave_as_geotiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_prob_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'probability/-104.884360899962_38.8362801837095.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0msave_as_geotiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_predict_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'mask/-104.884360899962_38.8362801837095.tif'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1/2545269990.py\u001b[0m in \u001b[0;36msave_as_geotiff\u001b[0;34m(data_array, file_name, transform, crs)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mcrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             transform=transform) as dst:\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m                            \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                            \u001b[0msharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 raise DriverCapabilityError(\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetWriterBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: Attempt to create new tiff file 'satellite_images/predict/probability/-104.884360899962_38.8362801837095.tif' failed: No such file or directory"
     ]
    }
   ],
   "source": [
    "def save_as_geotiff(data_array, file_name, transform, crs):\n",
    "    with rasterio.open(\n",
    "            file_name, \n",
    "            'w', \n",
    "            driver='GTiff', \n",
    "            height=data_array.shape[0],\n",
    "            width=data_array.shape[1], \n",
    "            count=1, \n",
    "            dtype=data_array.dtype,\n",
    "            crs=crs,\n",
    "            transform=transform) as dst:\n",
    "        dst.write(data_array, 1)\n",
    "\n",
    "# Save the mask probability and prediction\n",
    "predict_path='satellite_images/predict/'\n",
    "\n",
    "save_as_geotiff(mask_prob_array, predict_path+'probability/-104.884360899962_38.8362801837095.tif', transform, crs)\n",
    "save_as_geotiff(mask_predict_array, predict_path+'mask/-104.884360899962_38.8362801837095.tif', transform, crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Images to the Google Cloud Bucket\n",
    "!gsutil -m cp -r {'satellite_images/predict'} gs://meter-sam/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Undeploy the model\n",
    "\n",
    "When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "Set `delete_bucket` to **True** to delete the Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [],
   "source": [
    "delete_bucket = False\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "model.delete()\n",
    "\n",
    "# Delete the endpoint using the Vertex endpoint object\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the custom trainig job\n",
    "job.delete()\n",
    "\n",
    "# Delete artifact repository\n",
    "! gcloud artifacts repositories delete $REPOSITORY --location=$REGION --quiet\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk-custom-image-classification-custom-container.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "PyTorch 2.0 (Local)",
   "language": "python",
   "name": "local-pytorch-2-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
